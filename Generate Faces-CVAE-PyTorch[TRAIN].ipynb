{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Faces with CVAE in PyTorch [TRAIN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import math\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformObj = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = \"./celeba/\"\n",
    "\n",
    "dataset = datasets.ImageFolder(root=dataroot, transform=transformObj)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_size=100):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        self.l1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
    "        self.l1b = nn.BatchNorm2d(32)\n",
    "        self.l2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
    "        self.l2b = nn.BatchNorm2d(64)\n",
    "        self.l3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1)\n",
    "        self.l3b = nn.BatchNorm2d(128)\n",
    "        self.l4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1)\n",
    "        self.l4b = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.l41 = nn.Linear(256*4*4, self.latent_size)\n",
    "        self.l42 = nn.Linear(256*4*4, self.latent_size)\n",
    "        \n",
    "        self.f = nn.Linear(self.latent_size, 256*4*4)\n",
    "        \n",
    "        self.l5 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1)\n",
    "        self.l6 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
    "        self.l7 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
    "        self.l8 = nn.ConvTranspose2d(in_channels=32, out_channels=3, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "    def encoder(self, x_in):\n",
    "        h = F.leaky_relu(self.l1b(self.l1(x_in)))\n",
    "        h = F.leaky_relu(self.l2b(self.l2(h)))\n",
    "        h = F.leaky_relu(self.l3b(self.l3(h)))\n",
    "        h = F.leaky_relu(self.l4b(self.l4(h)))\n",
    "        \n",
    "        h = h.view(h.size(0), -1)\n",
    "        \n",
    "        return self.l41(h), self.l42(h)\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        z = self.f(z)\n",
    "        z = z.view(-1, 256, 4, 4)\n",
    "        \n",
    "        z = F.leaky_relu(self.l5(z))\n",
    "        z = F.leaky_relu(self.l6(z))\n",
    "        z = F.leaky_relu(self.l7(z))\n",
    "        z = torch.sigmoid(self.l8(z))\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return torch.add(eps.mul(std), mu)\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        mu, log_var = self.encoder(x_in)\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (l1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (l1b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (l2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (l2b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (l3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (l3b): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (l4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (l4b): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (l41): Linear(in_features=4096, out_features=100, bias=True)\n",
       "  (l42): Linear(in_features=4096, out_features=100, bias=True)\n",
       "  (f): Linear(in_features=100, out_features=4096, bias=True)\n",
       "  (l5): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (l6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (l7): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (l8): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VAE()\n",
    "    \n",
    "vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 32, 32]           1,568\n",
      "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
      "            Conv2d-3           [-1, 64, 16, 16]          32,832\n",
      "       BatchNorm2d-4           [-1, 64, 16, 16]             128\n",
      "            Conv2d-5            [-1, 128, 8, 8]         131,200\n",
      "       BatchNorm2d-6            [-1, 128, 8, 8]             256\n",
      "            Conv2d-7            [-1, 256, 4, 4]         524,544\n",
      "       BatchNorm2d-8            [-1, 256, 4, 4]             512\n",
      "            Linear-9                  [-1, 100]         409,700\n",
      "           Linear-10                  [-1, 100]         409,700\n",
      "           Linear-11                 [-1, 4096]         413,696\n",
      "  ConvTranspose2d-12            [-1, 128, 8, 8]         524,416\n",
      "  ConvTranspose2d-13           [-1, 64, 16, 16]         131,136\n",
      "  ConvTranspose2d-14           [-1, 32, 32, 32]          32,800\n",
      "  ConvTranspose2d-15            [-1, 3, 64, 64]           1,539\n",
      "================================================================\n",
      "Total params: 2,614,091\n",
      "Trainable params: 2,614,091\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 1.50\n",
      "Params size (MB): 9.97\n",
      "Estimated Total Size (MB): 11.52\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(vae, (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG_model, self).__init__()\n",
    "        self.model_layers = models.vgg.vgg19(pretrained=True).features\n",
    "        self.content_layers = [\"31\", \"33\", \"35\"]\n",
    "        \n",
    "        for parameter in self.model_layers.parameters():\n",
    "            parameter.requires_grad = False\n",
    "        \n",
    "    def forward(self, image):\n",
    "        batch_size = image.size(0)\n",
    "        output = image\n",
    "        output_layers = []\n",
    "        for name, module in self.model_layers.named_children():\n",
    "            output = module(output)\n",
    "            if name in self.content_layers:\n",
    "                output_layers.append(output.view(batch_size, -1))\n",
    "        return output_layers\n",
    "    \n",
    "    def feature_perceptual_loss(self, recon_x, x):\n",
    "        total_loss = 0\n",
    "        for x1, x2 in zip(recon_x, x):\n",
    "            total_loss += F.mse_loss(x1, x2)\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters(), lr=0.0005)\n",
    "\n",
    "def loss_function(recon_x, x, mu, log_var, vgg_model):\n",
    "    \n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    #MSL = F.mse_loss(recon_x, x)\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) # KL Divergence from MIT 6.S191\n",
    "    #return (MSL + KLD)\n",
    "    \n",
    "    x_f = vgg_model(x)\n",
    "    recon_x_f = vgg_model(recon_x)\n",
    "    FPL = vgg_model.feature_perceptual_loss(recon_x_f, x_f)\n",
    "    \n",
    "    return (BCE + KLD + FPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    vae.train()\n",
    "    \n",
    "    vgg = VGG_model().to(device)\n",
    "\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(dataloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        r_batch, mu, log_var = vae(data)\n",
    "\n",
    "        loss = loss_function(r_batch, data, mu, log_var, vgg)\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx%2500==0:\n",
    "            print(\"Batch no. finished in Epoch: \", batch_idx)\n",
    "\n",
    "    print('Epoch: {} Train mean loss: {:.8f}'.format(epoch, train_loss / len(dataloader.dataset)))\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch no. finished in Epoch:  0\n",
      "Batch no. finished in Epoch:  100\n",
      "Batch no. finished in Epoch:  200\n",
      "Batch no. finished in Epoch:  300\n",
      "Batch no. finished in Epoch:  400\n",
      "Batch no. finished in Epoch:  500\n",
      "Batch no. finished in Epoch:  600\n",
      "Batch no. finished in Epoch:  700\n",
      "Batch no. finished in Epoch:  800\n",
      "Batch no. finished in Epoch:  900\n",
      "Batch no. finished in Epoch:  1000\n",
      "Batch no. finished in Epoch:  1100\n",
      "Batch no. finished in Epoch:  1200\n",
      "Batch no. finished in Epoch:  1300\n",
      "Batch no. finished in Epoch:  1400\n",
      "Batch no. finished in Epoch:  1500\n",
      "Batch no. finished in Epoch:  1600\n",
      "Batch no. finished in Epoch:  1700\n",
      "Batch no. finished in Epoch:  1800\n",
      "Batch no. finished in Epoch:  1900\n",
      "Batch no. finished in Epoch:  2000\n",
      "Batch no. finished in Epoch:  2100\n",
      "Batch no. finished in Epoch:  2200\n",
      "Batch no. finished in Epoch:  2300\n",
      "Batch no. finished in Epoch:  2400\n",
      "Batch no. finished in Epoch:  2500\n",
      "Batch no. finished in Epoch:  2600\n",
      "Batch no. finished in Epoch:  2700\n",
      "Batch no. finished in Epoch:  2800\n",
      "Batch no. finished in Epoch:  2900\n",
      "Batch no. finished in Epoch:  3000\n",
      "Batch no. finished in Epoch:  3100\n",
      "Batch no. finished in Epoch:  3200\n",
      "Batch no. finished in Epoch:  3300\n",
      "Batch no. finished in Epoch:  3400\n",
      "Batch no. finished in Epoch:  3500\n",
      "Batch no. finished in Epoch:  3600\n",
      "Batch no. finished in Epoch:  3700\n",
      "Batch no. finished in Epoch:  3800\n",
      "Batch no. finished in Epoch:  3900\n",
      "Batch no. finished in Epoch:  4000\n",
      "Batch no. finished in Epoch:  4100\n",
      "Batch no. finished in Epoch:  4200\n",
      "Batch no. finished in Epoch:  4300\n",
      "Batch no. finished in Epoch:  4400\n",
      "Batch no. finished in Epoch:  4500\n",
      "Batch no. finished in Epoch:  4600\n",
      "Batch no. finished in Epoch:  4700\n",
      "Batch no. finished in Epoch:  4800\n",
      "Batch no. finished in Epoch:  4900\n",
      "Batch no. finished in Epoch:  5000\n",
      "Batch no. finished in Epoch:  5100\n",
      "Batch no. finished in Epoch:  5200\n",
      "Batch no. finished in Epoch:  5300\n",
      "Batch no. finished in Epoch:  5400\n",
      "Batch no. finished in Epoch:  5500\n",
      "Batch no. finished in Epoch:  5600\n",
      "Batch no. finished in Epoch:  5700\n",
      "Batch no. finished in Epoch:  5800\n",
      "Batch no. finished in Epoch:  5900\n",
      "Batch no. finished in Epoch:  6000\n",
      "Batch no. finished in Epoch:  6100\n",
      "Batch no. finished in Epoch:  6200\n",
      "Batch no. finished in Epoch:  6300\n",
      "Batch no. finished in Epoch:  6400\n",
      "Batch no. finished in Epoch:  6500\n",
      "Batch no. finished in Epoch:  6600\n",
      "Batch no. finished in Epoch:  6700\n",
      "Batch no. finished in Epoch:  6800\n",
      "Batch no. finished in Epoch:  6900\n",
      "Batch no. finished in Epoch:  7000\n",
      "Batch no. finished in Epoch:  7100\n",
      "Batch no. finished in Epoch:  7200\n",
      "Batch no. finished in Epoch:  7300\n",
      "Batch no. finished in Epoch:  7400\n",
      "Batch no. finished in Epoch:  7500\n",
      "Batch no. finished in Epoch:  7600\n",
      "Batch no. finished in Epoch:  7700\n",
      "Batch no. finished in Epoch:  7800\n",
      "Batch no. finished in Epoch:  7900\n",
      "Batch no. finished in Epoch:  8000\n",
      "Batch no. finished in Epoch:  8100\n",
      "Batch no. finished in Epoch:  8200\n",
      "Batch no. finished in Epoch:  8300\n",
      "Batch no. finished in Epoch:  8400\n",
      "Batch no. finished in Epoch:  8500\n",
      "Batch no. finished in Epoch:  8600\n",
      "Batch no. finished in Epoch:  8700\n",
      "Batch no. finished in Epoch:  8800\n",
      "Batch no. finished in Epoch:  8900\n",
      "Batch no. finished in Epoch:  9000\n",
      "Batch no. finished in Epoch:  9100\n",
      "Batch no. finished in Epoch:  9200\n",
      "Batch no. finished in Epoch:  9300\n",
      "Batch no. finished in Epoch:  9400\n",
      "Batch no. finished in Epoch:  9500\n",
      "Batch no. finished in Epoch:  9600\n",
      "Batch no. finished in Epoch:  9700\n",
      "Batch no. finished in Epoch:  9800\n",
      "Batch no. finished in Epoch:  9900\n",
      "Batch no. finished in Epoch:  10000\n",
      "Batch no. finished in Epoch:  10100\n",
      "Batch no. finished in Epoch:  10200\n",
      "Batch no. finished in Epoch:  10300\n",
      "Batch no. finished in Epoch:  10400\n",
      "Batch no. finished in Epoch:  10500\n",
      "Batch no. finished in Epoch:  10600\n",
      "Batch no. finished in Epoch:  10700\n",
      "Batch no. finished in Epoch:  10800\n",
      "Batch no. finished in Epoch:  10900\n",
      "Batch no. finished in Epoch:  11000\n",
      "Batch no. finished in Epoch:  11100\n",
      "Batch no. finished in Epoch:  11200\n",
      "Batch no. finished in Epoch:  11300\n",
      "Batch no. finished in Epoch:  11400\n",
      "Batch no. finished in Epoch:  11500\n",
      "Batch no. finished in Epoch:  11600\n",
      "Batch no. finished in Epoch:  11700\n",
      "Batch no. finished in Epoch:  11800\n",
      "Batch no. finished in Epoch:  11900\n",
      "Batch no. finished in Epoch:  12000\n",
      "Batch no. finished in Epoch:  12100\n",
      "Batch no. finished in Epoch:  12200\n",
      "Batch no. finished in Epoch:  12300\n",
      "Batch no. finished in Epoch:  12400\n",
      "Batch no. finished in Epoch:  12500\n",
      "Batch no. finished in Epoch:  12600\n",
      "Epoch: 1 Train mean loss: 6440.46855133\n"
     ]
    }
   ],
   "source": [
    "n_epoches = 1\n",
    "\n",
    "loss_hist = []\n",
    "\n",
    "for epoch in range(1, n_epoches+1):\n",
    "    loss_epoch = train(epoch)\n",
    "    loss_hist.append(loss_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    counter = 0\n",
    "    for i in range(100): \n",
    "        counter += 1\n",
    "        z = (torch.rand(100)*256).to(device)\n",
    "        sample = vae.decoder(z).to(device)\n",
    "        save_image(sample.view(3, 64, 64), './samplesFACES/sample' + str(counter) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), \"./model_weight/wf.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_hist)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
